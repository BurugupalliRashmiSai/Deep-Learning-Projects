{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2z-mZnNajlnk"
   },
   "source": [
    "133008100 DL CSCE 636 Project-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions to run code\n",
    "\n",
    "1. Run \"preprocess_data\" function before running the model on test data. - X_test=preprocess_data(X_test)\n",
    "2. And then its a direct process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSsmlZY-gMUZ"
   },
   "source": [
    "Load Training data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Y0C7uiy_cb2Q"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "l8sd-IfIb34e"
   },
   "outputs": [],
   "source": [
    "train_data_path=\"./636_project1_train_images\"\n",
    "train_labels_path=\"./636_project1_train_labels\"\n",
    "\n",
    "X_train=pickle.load(open(train_data_path, 'rb')) \n",
    "y_train=pickle.load(open(train_labels_path, 'rb')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jn1-QLGFg5bH"
   },
   "source": [
    "Before training, I preprocessed the data by reshaping it into the shape the network\n",
    "expects and scaling it so that all values are in the [0, 1] interval. The training\n",
    "images, for instance, were stored in an array of shape (60000, 28, 28) of type\n",
    "uint8 with values in the [0, 255] interval. We transform it into a float32 array of\n",
    "shape (60000, 28 * 28) with values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X_train):\n",
    "    X_train = X_train.reshape((60000, 28, 28, 1))\n",
    "    X_train = X_train.astype(\"float32\") / 255\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i1eGsLF7cTfd",
    "outputId": "112f2272-5bda-4c0b-ceed-e09d2594146d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([60000, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VEoPjS2XcUM-",
    "outputId": "c85f847e-0c11-4190-e031-2d1f99458995"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([60000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "set(np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6, shape=(), dtype=uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "digit = X_train[103]\n",
    "print(y_train[103])\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=preprocess_data(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of noisy image data, Conv2DTranspose can be used to recover the original image from the noisy image. The layer learns to reverse the effects of noise by learning a mapping from the noisy image to the original image. The deconvolution process helps in removing the noise and enhancing the details in the image.\n",
    "\n",
    "The Conv2DTranspose layer works by performing a convolution operation on the input with a learned set of filters, followed by a scaling operation to upsample the output to a larger size. The output of the layer is fed into subsequent layers for further processing.\n",
    "\n",
    "Batch normalization helps in stabilizing the input to each layer of the neural network, making the training process faster and more efficient. It can also help in reducing the effect of noise on the model by normalizing the inputs to each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "def create_model(inputs,outputs):\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JnGY861Mfn-4",
    "outputId": "de4fb2ec-c60b-4a40-bd70-bb3820c25799",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 26, 26, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 13, 13, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                11530     \n",
      "=================================================================\n",
      "Total params: 123,242\n",
      "Trainable params: 123,114\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kfold.split() method generates indices to split the data into training and test sets for each fold of the cross-validation. The model.fit() method is then used to train the model on training data for each fold again and again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using callbacks to act on a model during training for earlystopping and model check point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2agV2-rVc5nE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "176/176 [==============================] - 60s 338ms/step - loss: 2.1158 - accuracy: 0.2295 - val_loss: 2.2621 - val_accuracy: 0.1869\n",
      "Epoch 2/20\n",
      "176/176 [==============================] - 58s 331ms/step - loss: 1.7229 - accuracy: 0.3969 - val_loss: 2.0900 - val_accuracy: 0.3043\n",
      "Epoch 3/20\n",
      "176/176 [==============================] - 58s 329ms/step - loss: 1.4612 - accuracy: 0.4949 - val_loss: 1.7633 - val_accuracy: 0.4975\n",
      "Epoch 4/20\n",
      "176/176 [==============================] - 59s 334ms/step - loss: 1.3507 - accuracy: 0.5370 - val_loss: 1.5119 - val_accuracy: 0.5082\n",
      "Epoch 5/20\n",
      "176/176 [==============================] - 58s 331ms/step - loss: 1.2863 - accuracy: 0.5571 - val_loss: 1.2384 - val_accuracy: 0.5874\n",
      "Epoch 6/20\n",
      "176/176 [==============================] - 58s 329ms/step - loss: 1.2283 - accuracy: 0.5767 - val_loss: 1.1721 - val_accuracy: 0.5955\n",
      "Epoch 7/20\n",
      "176/176 [==============================] - 58s 327ms/step - loss: 1.1885 - accuracy: 0.5942 - val_loss: 1.1375 - val_accuracy: 0.6185\n",
      "Epoch 8/20\n",
      "176/176 [==============================] - 58s 328ms/step - loss: 1.1706 - accuracy: 0.5984 - val_loss: 1.0877 - val_accuracy: 0.6295\n",
      "Epoch 9/20\n",
      "176/176 [==============================] - 58s 330ms/step - loss: 1.1528 - accuracy: 0.6036 - val_loss: 1.0949 - val_accuracy: 0.6265\n",
      "Epoch 1/20\n",
      "176/176 [==============================] - 59s 334ms/step - loss: 1.1409 - accuracy: 0.6076 - val_loss: 1.0079 - val_accuracy: 0.6587\n",
      "Epoch 2/20\n",
      "176/176 [==============================] - 57s 323ms/step - loss: 1.1094 - accuracy: 0.6197 - val_loss: 0.9954 - val_accuracy: 0.6553\n",
      "Epoch 1/20\n",
      "176/176 [==============================] - 59s 331ms/step - loss: 1.0991 - accuracy: 0.6255 - val_loss: 0.9757 - val_accuracy: 0.6643\n",
      "Epoch 2/20\n",
      "176/176 [==============================] - 57s 326ms/step - loss: 1.0765 - accuracy: 0.6300 - val_loss: 0.9698 - val_accuracy: 0.6679\n",
      "Epoch 3/20\n",
      "176/176 [==============================] - 58s 328ms/step - loss: 1.0651 - accuracy: 0.6352 - val_loss: 0.9602 - val_accuracy: 0.6647\n",
      "Epoch 1/20\n",
      "176/176 [==============================] - 59s 331ms/step - loss: 1.0886 - accuracy: 0.6256 - val_loss: 0.9271 - val_accuracy: 0.6808\n",
      "Epoch 2/20\n",
      "176/176 [==============================] - 58s 328ms/step - loss: 1.0557 - accuracy: 0.6372 - val_loss: 0.9373 - val_accuracy: 0.6789\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=4, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=1,\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "    filepath='Final_model_cv_new.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "for train, test in kfold.split(X_train, y_train): \n",
    "    model = create_model(inputs,outputs)\n",
    "    history = model.fit(X_train[train], \n",
    "            y_train[train],\n",
    "            epochs=20, \n",
    "            validation_data=(X_train[test], y_train[test]),\n",
    "            callbacks=callbacks_list,\n",
    "            batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
